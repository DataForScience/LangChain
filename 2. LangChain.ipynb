{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>LangChain for Generative AI</h1>\n",
    "<h1>LangChain</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results\n",
    "\n",
    "\n",
    "import langchain\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "import langchain_openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import langchain_anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "import langchain_core\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import langchain_community\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by print out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.3\n",
      "IPython version      : 9.2.0\n",
      "\n",
      "Compiler    : Clang 17.0.0 (clang-1700.0.13.3)\n",
      "OS          : Darwin\n",
      "Release     : 25.2.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 59665e09cdb9e15991a6739b876a356cbd24f05a\n",
      "\n",
      "matplotlib         : 3.10.3\n",
      "langchain_openai   : 0.3.18\n",
      "langchain_community: 0.3.24\n",
      "langchain_anthropic: 0.3.14\n",
      "langchain_core     : 0.3.62\n",
      "numpy              : 2.2.5\n",
      "watermark          : 2.5.0\n",
      "openai             : 1.78.1\n",
      "transformers       : 4.52.3\n",
      "pandas             : 2.2.3\n",
      "langchain          : 0.3.25\n",
      "torch              : 2.7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is generate API key on the OpenAI website and store it as the \"OPENAI_API_KEY\" variable in your local environment. Without it we won't be able to do anything. You can find your API key in your using settings: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key. Then we are ready to instantiate the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-5.2-codex\n",
      "gpt-4o-mini-tts-2025-12-15\n",
      "gpt-realtime-mini-2025-12-15\n",
      "gpt-audio-mini-2025-12-15\n",
      "chatgpt-image-latest\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "computer-use-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-2024-11-20\n",
      "computer-use-preview-2025-03-11\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "o1-pro-2025-03-19\n",
      "o1-pro\n",
      "gpt-4o-mini-tts\n",
      "o3-2025-04-16\n",
      "o4-mini-2025-04-16\n",
      "o3\n",
      "o4-mini\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "codex-mini-latest\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "o4-mini-deep-research\n",
      "gpt-4o-transcribe-diarize\n",
      "o4-mini-deep-research-2025-06-26\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-audio-2025-08-28\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-audio\n",
      "gpt-5-codex\n",
      "gpt-image-1-mini\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-5-search-api\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-search-api-2025-10-14\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5.1-codex-max\n",
      "gpt-image-1.5\n",
      "gpt-5.2-2025-12-11\n",
      "gpt-5.2\n",
      "gpt-5.2-pro-2025-12-11\n",
      "gpt-5.2-pro\n",
      "gpt-5.2-chat-latest\n",
      "gpt-4o-mini-transcribe-2025-12-15\n",
      "gpt-4o-mini-transcribe-2025-03-20\n",
      "gpt-4o-mini-tts-2025-03-20\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1\n",
      "whisper-1\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([model.id for model in client.models.list().data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What was Superman's weakness?\"\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-D0SRzbrk7MdxURiwz528cyMJazxAD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Superman's most well-known weakness is kryptonite, a mineral from his home planet of Krypton. Exposure to kryptonite weakens Superman and can be lethal with prolonged exposure. There are different types of kryptonite, each having distinct effects on him; the most common is green kryptonite, which drains his powers and can potentially kill him. Other weaknesses include red solar radiation, which strips him of his powers by mimicking the sun of Krypton, and magic, which can affect him like it would any ordinary person.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1769001735, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_deacdd5f6f', usage=CompletionUsage(completion_tokens=104, prompt_tokens=13, total_tokens=117, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superman's most well-known weakness is kryptonite, a mineral from his home planet of Krypton. Exposure to kryptonite weakens Superman and can be lethal with prolonged exposure. There are different types of kryptonite, each having distinct effects on him; the most common is green kryptonite, which drains his powers and can potentially kill him. Other weaknesses include red solar radiation, which strips him of his powers by mimicking the sun of Krypton, and magic, which can affect him like it would any ordinary person.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the LangChain interface for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Superman's primary weakness is Kryptonite, a mineral from his home planet, Krypton. Exposure to Kryptonite can weaken Superman, stripping him of his powers, and prolonged exposure can be fatal. Kryptonite comes in various forms, with green Kryptonite being the most common and well-known version that affects Superman.\\n\\nIn addition to Kryptonite, Superman is also vulnerable to red solar radiation, which can depower him by mimicking the radiation of his home planet's red sun. Magic is another vulnerability; Superman's invulnerability does not protect him against magical forces and spells. While not a traditional weakness, Superman's strong ethical code and his dedication to protecting others can sometimes be used against him by his enemies.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 13, 'total_tokens': 154, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-D0SS2CBYcUsflT8kQCnFy8sChhDwz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e9fd3792-a5d5-4b3a-8a02-f38a687bae3e-0' usage_metadata={'input_tokens': 13, 'output_tokens': 141, 'total_tokens': 154, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"What was Superman's weakness?\"),\n",
    "]\n",
    "\n",
    "output = model.invoke(messages)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 141,\n",
       " 'prompt_tokens': 13,\n",
       " 'total_tokens': 154,\n",
       " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "  'audio_tokens': 0,\n",
       "  'reasoning_tokens': 0,\n",
       "  'rejected_prediction_tokens': 0},\n",
       " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.response_metadata[\"token_usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Superman's primary weakness is kryptonite, a radioactive fragment from his home planet of Krypton. Exposure to kryptonite weakens him, drains his powers, and can ultimately be lethal if he is in close contact with it for an extended period. Additionally, Superman is also vulnerable to magic and red solar radiation, which can neutralize his abilities. Beyond these, he has the same vulnerabilities as any human being, such as emotional and moral challenges, which can affect him deeply.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our first chain. Stages of the chain are conencted with the pipe '|' character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whenver we call __invoke()__ on the chain, it automatically runs all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Superman's primary weakness is kryptonite, a mineral from his home planet of Krypton. When he is exposed to kryptonite, it weakens him significantly, stripping him of his powers and potentially leading to severe illness or even death with prolonged exposure. Different forms of kryptonite can have varying effects, but the most commonly depicted is green kryptonite. Aside from kryptonite, Superman's powers can be diminished under a red sun, which mimics the conditions of Krypton's original solar environment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create templates for our prompts, following conventions similar to the Jinja templating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following text into {language}:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can combine multiple messages into a single template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     (\"system\", system_template), \n",
    "     (\"user\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate the prompt, we must provide the correct fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following text into italian:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Be the change that you wish to see in the world.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"italian\", \n",
    "        \"text\": \"Be the change that you wish to see in the world.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full interaction is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following text into italian:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Be the change that you wish to see in the world.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sei addestrato sui dati fino a ottobre 2023.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"language\": \"italian\", \n",
    "    \"text\": \"Be the change that you wish to see in the world.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "ChatAnthropic(\n",
       "    *args: Any,\n",
       "    name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose: bool = <factory>,\n",
       "    callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    tags: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    metadata: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    custom_get_token_ids: Optional[Callable[[str], list[int]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    disable_streaming: Union[bool, Literal[\u001b[33m'tool_calling'\u001b[39m]] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    model_name: str,\n",
       "    max_tokens_to_sample: int = \u001b[32m1024\u001b[39m,\n",
       "    temperature: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    top_k: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    top_p: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    timeout: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_retries: int = \u001b[32m2\u001b[39m,\n",
       "    stop: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    base_url: Optional[str] = <factory>,\n",
       "    api_key: pydantic.types.SecretStr = <factory>,\n",
       "    default_headers: Optional[collections.abc.Mapping[str, str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    betas: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    model_kwargs: dict[str, typing.Any] = <factory>,\n",
       "    streaming: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    stream_usage: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    thinking: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    mcp_servers: Optional[list[dict[str, Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Anthropic chat models.\n",
       "\n",
       "See https://docs.anthropic.com/en/docs/models-overview for a list of the latest models.\n",
       "\n",
       "Setup:\n",
       "    Install ``langchain-anthropic`` and set environment variable ``ANTHROPIC_API_KEY``.\n",
       "\n",
       "    .. code-block:: bash\n",
       "\n",
       "        pip install -U langchain-anthropic\n",
       "        export ANTHROPIC_API_KEY=\"your-api-key\"\n",
       "\n",
       "Key init args — completion params:\n",
       "    model: str\n",
       "        Name of Anthropic model to use. E.g. \"claude-3-sonnet-20240229\".\n",
       "    temperature: float\n",
       "        Sampling temperature. Ranges from 0.0 to 1.0.\n",
       "    max_tokens: int\n",
       "        Max number of tokens to generate.\n",
       "\n",
       "Key init args — client params:\n",
       "    timeout: Optional[float]\n",
       "        Timeout for requests.\n",
       "    max_retries: int\n",
       "        Max number of retries if a request fails.\n",
       "    api_key: Optional[str]\n",
       "        Anthropic API key. If not passed in will be read from env var ANTHROPIC_API_KEY.\n",
       "    base_url: Optional[str]\n",
       "        Base URL for API requests. Only specify if using a proxy or service\n",
       "        emulator.\n",
       "\n",
       "See full list of supported init args and their descriptions in the params section.\n",
       "\n",
       "Instantiate:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "        llm = ChatAnthropic(\n",
       "            model=\"claude-3-sonnet-20240229\",\n",
       "            temperature=0,\n",
       "            max_tokens=1024,\n",
       "            timeout=None,\n",
       "            max_retries=2,\n",
       "            # api_key=\"...\",\n",
       "            # base_url=\"...\",\n",
       "            # other params...\n",
       "        )\n",
       "\n",
       "**NOTE**: Any param which is not explicitly supported will be passed directly to the\n",
       "``anthropic.Anthropic.messages.create(...)`` API every time to the model is\n",
       "invoked. For example:\n",
       "\n",
       ".. code-block:: python\n",
       "\n",
       "    from langchain_anthropic import ChatAnthropic\n",
       "    import anthropic\n",
       "\n",
       "    ChatAnthropic(..., extra_headers={}).invoke(...)\n",
       "\n",
       "    # results in underlying API call of:\n",
       "\n",
       "    anthropic.Anthropic(..).messages.create(..., extra_headers={})\n",
       "\n",
       "    # which is also equivalent to:\n",
       "\n",
       "    ChatAnthropic(...).invoke(..., extra_headers={})\n",
       "\n",
       "Invoke:\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
       "            (\"human\", \"I love programming.\"),\n",
       "        ]\n",
       "        llm.invoke(messages)\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n",
       "\n",
       "Stream:\n",
       "    .. code-block:: python\n",
       "\n",
       "        for chunk in llm.stream(messages):\n",
       "            print(chunk.text(), end=\"\")\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessageChunk(content='J', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content=\"'\", id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content='a', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content='ime', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content=' la', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content=' programm', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content='ation', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "        AIMessageChunk(content='.', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        stream = llm.stream(messages)\n",
       "        full = next(stream)\n",
       "        for chunk in stream:\n",
       "            full += chunk\n",
       "        full\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessageChunk(content=\"J'aime la programmation.\", id='run-b34faef0-882f-4869-a19c-ed2b856e6361')\n",
       "\n",
       "Async:\n",
       "    .. code-block:: python\n",
       "\n",
       "        await llm.ainvoke(messages)\n",
       "\n",
       "        # stream:\n",
       "        # async for chunk in (await llm.astream(messages))\n",
       "\n",
       "        # batch:\n",
       "        # await llm.abatch([messages])\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n",
       "\n",
       "Tool calling:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from pydantic import BaseModel, Field\n",
       "\n",
       "        class GetWeather(BaseModel):\n",
       "            '''Get the current weather in a given location'''\n",
       "\n",
       "            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
       "\n",
       "        class GetPopulation(BaseModel):\n",
       "            '''Get the current population in a given location'''\n",
       "\n",
       "            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
       "\n",
       "        llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
       "        ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n",
       "        ai_msg.tool_calls\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [{'name': 'GetWeather',\n",
       "          'args': {'location': 'Los Angeles, CA'},\n",
       "          'id': 'toolu_01KzpPEAgzura7hpBqwHbWdo'},\n",
       "         {'name': 'GetWeather',\n",
       "          'args': {'location': 'New York, NY'},\n",
       "          'id': 'toolu_01JtgbVGVJbiSwtZk3Uycezx'},\n",
       "         {'name': 'GetPopulation',\n",
       "          'args': {'location': 'Los Angeles, CA'},\n",
       "          'id': 'toolu_01429aygngesudV9nTbCKGuw'},\n",
       "         {'name': 'GetPopulation',\n",
       "          'args': {'location': 'New York, NY'},\n",
       "          'id': 'toolu_01JPktyd44tVMeBcPPnFSEJG'}]\n",
       "\n",
       "    See ``ChatAnthropic.bind_tools()`` method for more.\n",
       "\n",
       "Structured output:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from typing import Optional\n",
       "\n",
       "        from pydantic import BaseModel, Field\n",
       "\n",
       "        class Joke(BaseModel):\n",
       "            '''Joke to tell user.'''\n",
       "\n",
       "            setup: str = Field(description=\"The setup of the joke\")\n",
       "            punchline: str = Field(description=\"The punchline to the joke\")\n",
       "            rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
       "\n",
       "        structured_llm = llm.with_structured_output(Joke)\n",
       "        structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)\n",
       "\n",
       "    See ``ChatAnthropic.with_structured_output()`` for more.\n",
       "\n",
       "Image input:\n",
       "    See `multimodal guides <https://python.langchain.com/docs/how_to/multimodal_inputs/>`_\n",
       "    for more detail.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        import base64\n",
       "\n",
       "        import httpx\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "        from langchain_core.messages import HumanMessage\n",
       "\n",
       "        image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
       "        image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
       "\n",
       "        llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
       "        message = HumanMessage(\n",
       "            content=[\n",
       "                {\n",
       "                    \"type\": \"text\",\n",
       "                    \"text\": \"Can you highlight the differences between these two images?\",\n",
       "                },\n",
       "                {\n",
       "                    \"type\": \"image\",\n",
       "                    \"source_type\": \"base64\",\n",
       "                    \"data\": image_data,\n",
       "                    \"mime_type\": \"image/jpeg\",\n",
       "                },\n",
       "                {\n",
       "                    \"type\": \"image\",\n",
       "                    \"source_type\": \"url\",\n",
       "                    \"url\": image_url,\n",
       "                },\n",
       "            ],\n",
       "        )\n",
       "        ai_msg = llm.invoke([message])\n",
       "        ai_msg.content\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        \"After examining both images carefully, I can see that they are actually identical.\"\n",
       "\n",
       "    .. dropdown:: Files API\n",
       "\n",
       "        You can also pass in files that are managed through Anthropic's\n",
       "        `Files API <https://docs.anthropic.com/en/docs/build-with-claude/files>`_:\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "            llm = ChatAnthropic(\n",
       "                model=\"claude-sonnet-4-20250514\",\n",
       "                betas=[\"files-api-2025-04-14\"],\n",
       "            )\n",
       "            input_message = {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"text\",\n",
       "                        \"text\": \"Describe this document.\",\n",
       "                    },\n",
       "                    {\n",
       "                        \"type\": \"image\",\n",
       "                        \"source_type\": \"id\",\n",
       "                        \"id\": \"file_abc123...\",\n",
       "                    },\n",
       "                ],\n",
       "            }\n",
       "            llm.invoke([input_message])\n",
       "\n",
       "PDF input:\n",
       "    See `multimodal guides <https://python.langchain.com/docs/how_to/multimodal_inputs/>`_\n",
       "    for more detail.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from base64 import b64encode\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "        from langchain_core.messages import HumanMessage\n",
       "        import requests\n",
       "\n",
       "        url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n",
       "        data = b64encode(requests.get(url).content).decode()\n",
       "\n",
       "        llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
       "        ai_msg = llm.invoke(\n",
       "            [\n",
       "                HumanMessage(\n",
       "                    [\n",
       "                        \"Summarize this document.\",\n",
       "                        {\n",
       "                            \"type\": \"file\",\n",
       "                            \"source_type\": \"base64\",\n",
       "                            \"mime_type\": \"application/pdf\",\n",
       "                            \"data\": data,\n",
       "                        },\n",
       "                    ]\n",
       "                )\n",
       "            ]\n",
       "        )\n",
       "        ai_msg.content\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        \"This appears to be a simple document...\"\n",
       "\n",
       "    .. dropdown:: Files API\n",
       "\n",
       "        You can also pass in files that are managed through Anthropic's\n",
       "        `Files API <https://docs.anthropic.com/en/docs/build-with-claude/files>`_:\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "            llm = ChatAnthropic(\n",
       "                model=\"claude-sonnet-4-20250514\",\n",
       "                betas=[\"files-api-2025-04-14\"],\n",
       "            )\n",
       "            input_message = {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"text\",\n",
       "                        \"text\": \"Describe this document.\",\n",
       "                    },\n",
       "                    {\n",
       "                        \"type\": \"file\",\n",
       "                        \"source_type\": \"id\",\n",
       "                        \"id\": \"file_abc123...\",\n",
       "                    },\n",
       "                ],\n",
       "            }\n",
       "            llm.invoke([input_message])\n",
       "\n",
       "Extended thinking:\n",
       "    Claude 3.7 Sonnet supports an\n",
       "    `extended thinking <https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking>`_\n",
       "    feature, which will output the step-by-step reasoning process that led to its\n",
       "    final answer.\n",
       "\n",
       "    To use it, specify the `thinking` parameter when initializing `ChatAnthropic`.\n",
       "    It can also be passed in as a kwarg during invocation.\n",
       "\n",
       "    You will need to specify a token budget to use this feature. See usage example:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "        llm = ChatAnthropic(\n",
       "            model=\"claude-3-7-sonnet-latest\",\n",
       "            max_tokens=5000,\n",
       "            thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n",
       "        )\n",
       "\n",
       "        response = llm.invoke(\"What is the cube root of 50.653?\")\n",
       "        response.content\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [{'signature': '...', 'thinking': \"To find the cube root of 50.653...\", 'type': 'thinking'}, {'text': 'The cube root of 50.653 is ...', 'type': 'text'}]\n",
       "\n",
       "Citations:\n",
       "    Anthropic supports a\n",
       "    `citations <https://docs.anthropic.com/en/docs/build-with-claude/citations>`_\n",
       "    feature that lets Claude attach context to its answers based on source\n",
       "    documents supplied by the user. When\n",
       "    `document content blocks <https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types>`_\n",
       "    with ``\"citations\": {\"enabled\": True}`` are included in a query, Claude may\n",
       "    generate citations in its response.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "        llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n",
       "\n",
       "        messages = [\n",
       "            {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"document\",\n",
       "                        \"source\": {\n",
       "                            \"type\": \"text\",\n",
       "                            \"media_type\": \"text/plain\",\n",
       "                            \"data\": \"The grass is green. The sky is blue.\",\n",
       "                        },\n",
       "                        \"title\": \"My Document\",\n",
       "                        \"context\": \"This is a trustworthy document.\",\n",
       "                        \"citations\": {\"enabled\": True},\n",
       "                    },\n",
       "                    {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n",
       "                ],\n",
       "            }\n",
       "        ]\n",
       "        response = llm.invoke(messages)\n",
       "        response.content\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [{'text': 'Based on the document, ', 'type': 'text'},\n",
       "        {'text': 'the grass is green',\n",
       "        'type': 'text',\n",
       "        'citations': [{'type': 'char_location',\n",
       "            'cited_text': 'The grass is green. ',\n",
       "            'document_index': 0,\n",
       "            'document_title': 'My Document',\n",
       "            'start_char_index': 0,\n",
       "            'end_char_index': 20}]},\n",
       "        {'text': ', and ', 'type': 'text'},\n",
       "        {'text': 'the sky is blue',\n",
       "        'type': 'text',\n",
       "        'citations': [{'type': 'char_location',\n",
       "            'cited_text': 'The sky is blue.',\n",
       "            'document_index': 0,\n",
       "            'document_title': 'My Document',\n",
       "            'start_char_index': 20,\n",
       "            'end_char_index': 36}]},\n",
       "        {'text': '.', 'type': 'text'}]\n",
       "\n",
       "Token usage:\n",
       "    .. code-block:: python\n",
       "\n",
       "        ai_msg = llm.invoke(messages)\n",
       "        ai_msg.usage_metadata\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n",
       "\n",
       "    Message chunks containing token usage will be included during streaming by\n",
       "    default:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        stream = llm.stream(messages)\n",
       "        full = next(stream)\n",
       "        for chunk in stream:\n",
       "            full += chunk\n",
       "        full.usage_metadata\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n",
       "\n",
       "    These can be disabled by setting ``stream_usage=False`` in the stream method,\n",
       "    or by setting ``stream_usage=False`` when initializing ChatAnthropic.\n",
       "\n",
       "Prompt caching:\n",
       "    See LangChain `docs <https://python.langchain.com/docs/integrations/chat/anthropic/#built-in-tools>`_\n",
       "    for more detail.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "        llm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\n",
       "\n",
       "        messages = [\n",
       "            {\n",
       "                \"role\": \"system\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"text\",\n",
       "                        \"text\": \"Below is some long context:\",\n",
       "                    },\n",
       "                    {\n",
       "                        \"type\": \"text\",\n",
       "                        \"text\": f\"{long_text}\",\n",
       "                        \"cache_control\": {\"type\": \"ephemeral\"},\n",
       "                    },\n",
       "                ],\n",
       "            },\n",
       "            {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": \"What's that about?\",\n",
       "            },\n",
       "        ]\n",
       "\n",
       "        response = llm.invoke(messages)\n",
       "        response.usage_metadata[\"input_token_details\"]\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'cache_read': 0, 'cache_creation': 1458}\n",
       "\n",
       "    .. dropdown:: Extended caching\n",
       "\n",
       "        The cache lifetime is 5 minutes by default. If this is too short, you can\n",
       "        apply one hour caching by enabling the ``\"extended-cache-ttl-2025-04-11\"``\n",
       "        beta header:\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            llm = ChatAnthropic(\n",
       "                model=\"claude-3-7-sonnet-20250219\",\n",
       "                betas=[\"extended-cache-ttl-2025-04-11\"],\n",
       "            )\n",
       "\n",
       "        and specifying ``\"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}``.\n",
       "\n",
       "        See `Claude documentation <https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration-beta>`_\n",
       "        for detail.\n",
       "\n",
       "Token-efficient tool use (beta):\n",
       "    See LangChain `docs <https://python.langchain.com/docs/integrations/chat/anthropic/>`_\n",
       "    for more detail.\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_anthropic import ChatAnthropic\n",
       "        from langchain_core.tools import tool\n",
       "\n",
       "        llm = ChatAnthropic(\n",
       "            model=\"claude-3-7-sonnet-20250219\",\n",
       "            temperature=0,\n",
       "            model_kwargs={\n",
       "                \"extra_headers\": {\n",
       "                    \"anthropic-beta\": \"token-efficient-tools-2025-02-19\"\n",
       "                }\n",
       "            }\n",
       "        )\n",
       "\n",
       "        @tool\n",
       "        def get_weather(location: str) -> str:\n",
       "            \"\"\"Get the weather at a location.\"\"\"\n",
       "            return \"It's sunny.\"\n",
       "\n",
       "        llm_with_tools = llm.bind_tools([get_weather])\n",
       "        response = llm_with_tools.invoke(\n",
       "            \"What's the weather in San Francisco?\"\n",
       "        )\n",
       "        print(response.tool_calls)\n",
       "        print(f'Total tokens: {response.usage_metadata[\"total_tokens\"]}')\n",
       "\n",
       "    .. code-block:: none\n",
       "\n",
       "        [{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01HLjQMSb1nWmgevQUtEyz17', 'type': 'tool_call'}]\n",
       "\n",
       "        Total tokens: 408\n",
       "\n",
       "Built-in tools:\n",
       "    See LangChain `docs <https://python.langchain.com/docs/integrations/chat/anthropic/>`_\n",
       "    for more detail.\n",
       "\n",
       "    .. dropdown::  Web search\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "            llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
       "\n",
       "            tool = {\"type\": \"web_search_20250305\", \"name\": \"web_search\", \"max_uses\": 3}\n",
       "            llm_with_tools = llm.bind_tools([tool])\n",
       "\n",
       "            response = llm_with_tools.invoke(\n",
       "                \"How do I update a web app to TypeScript 5.5?\"\n",
       "            )\n",
       "\n",
       "    .. dropdown::  Code execution\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            llm = ChatAnthropic(\n",
       "                model=\"claude-sonnet-4-20250514\",\n",
       "                betas=[\"code-execution-2025-05-22\"],\n",
       "            )\n",
       "\n",
       "            tool = {\"type\": \"code_execution_20250522\", \"name\": \"code_execution\"}\n",
       "            llm_with_tools = llm.bind_tools([tool])\n",
       "\n",
       "            response = llm_with_tools.invoke(\n",
       "                \"Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\"\n",
       "            )\n",
       "\n",
       "    .. dropdown::  Remote MCP\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "            mcp_servers = [\n",
       "                {\n",
       "                    \"type\": \"url\",\n",
       "                    \"url\": \"https://mcp.deepwiki.com/mcp\",\n",
       "                    \"name\": \"deepwiki\",\n",
       "                    \"tool_configuration\": {  # optional configuration\n",
       "                        \"enabled\": True,\n",
       "                        \"allowed_tools\": [\"ask_question\"],\n",
       "                    },\n",
       "                    \"authorization_token\": \"PLACEHOLDER\",  # optional authorization\n",
       "                }\n",
       "            ]\n",
       "\n",
       "            llm = ChatAnthropic(\n",
       "                model=\"claude-sonnet-4-20250514\",\n",
       "                betas=[\"mcp-client-2025-04-04\"],\n",
       "                mcp_servers=mcp_servers,\n",
       "            )\n",
       "\n",
       "            response = llm.invoke(\n",
       "                \"What transport protocols does the 2025-03-26 version of the MCP \"\n",
       "                \"spec (modelcontextprotocol/modelcontextprotocol) support?\"\n",
       "            )\n",
       "\n",
       "    .. dropdown::  Text editor\n",
       "\n",
       "        .. code-block:: python\n",
       "\n",
       "            from langchain_anthropic import ChatAnthropic\n",
       "\n",
       "            llm = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\")\n",
       "\n",
       "            tool = {\"type\": \"text_editor_20250124\", \"name\": \"str_replace_editor\"}\n",
       "            llm_with_tools = llm.bind_tools([tool])\n",
       "\n",
       "            response = llm_with_tools.invoke(\n",
       "                \"There's a syntax error in my primes.py file. Can you help me fix it?\"\n",
       "            )\n",
       "            print(response.text())\n",
       "            response.tool_calls\n",
       "\n",
       "        .. code-block:: none\n",
       "\n",
       "            I'd be happy to help you fix the syntax error in your primes.py file. First, let's look at the current content of the file to identify the error.\n",
       "\n",
       "            [{'name': 'str_replace_editor',\n",
       "            'args': {'command': 'view', 'path': '/repo/primes.py'},\n",
       "            'id': 'toolu_01VdNgt1YV7kGfj9LFLm6HyQ',\n",
       "            'type': 'tool_call'}]\n",
       "\n",
       "Response metadata\n",
       "    .. code-block:: python\n",
       "\n",
       "        ai_msg = llm.invoke(messages)\n",
       "        ai_msg.response_metadata\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'id': 'msg_013xU6FHEGEq76aP4RgFerVT',\n",
       "         'model': 'claude-3-sonnet-20240229',\n",
       "         'stop_reason': 'end_turn',\n",
       "         'stop_sequence': None,\n",
       "         'usage': {'input_tokens': 25, 'output_tokens': 11}}\n",
       "\u001b[31mFile:\u001b[39m           ~/My Drive/DataForScience/LangChain/.venv/lib/python3.13/site-packages/langchain_anthropic/chat_models.py\n",
       "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
       "\u001b[31mSubclasses:\u001b[39m     ChatAnthropicMessages"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = ChatAnthropic(model=\"claude-haiku-4-5-20251001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Superman's primary weakness is **kryptonite**, radioactive fragments from his home planet Krypton. Exposure to it weakens or can kill him.\\n\\nHe also has other vulnerabilities:\\n\\n- **Magic** - he has no special resistance to magical attacks\\n- **Red sun radiation** - rays from a red sun (unlike Earth's yellow sun that powers him) drain his abilities\\n- **Psychological weaknesses** - his love for people he cares about can be exploited; his strong moral code sometimes limits his actions\\n\\nKryptonite remains his most famous and commonly referenced weakness across Superman stories.\", additional_kwargs={}, response_metadata={'id': 'msg_01QzWrkYbVQE14gedZcfRasG', 'model': 'claude-haiku-4-5-20251001', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 13, 'output_tokens': 133, 'server_tool_use': None, 'service_tier': 'standard', 'cache_creation': {'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}, 'model_name': 'claude-haiku-4-5-20251001'}, id='run--86d1f26a-318c-42f1-882f-8cefffb4b0b7-0', usage_metadata={'input_tokens': 13, 'output_tokens': 133, 'total_tokens': 146, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.invoke(\"What is Superman's weakness?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_a = prompt_template | model_a | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatAnthropic(model='claude-haiku-4-5-20251001', anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Sii il cambiamento che desideri vedere nel mondo.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_a.invoke(\n",
    "    {\n",
    "        \"language\": \"italian\", \n",
    "        \"text\": \"Be the change that you wish to see in the world.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bruno! How can I assist you today?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bruno\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You just mentioned that your name is Bruno. How can I help you further?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc2': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Bruno\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bruno! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-D0SSGQZST5oZUF2SLJrRGytYpEgR4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3889f24-3e93-40bc-a8ba-04789a1d3e1b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='You just mentioned that your name is Bruno. How can I help you further?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 33, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'id': 'chatcmpl-D0SSH9JyBiadK0RwQ4LAzzT9OwIcn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--837e471b-bdb2-4a0d-9f3c-e134dfa4b893-0', usage_metadata={'input_tokens': 33, 'output_tokens': 16, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I can't determine your name based on this information.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Bruno. If there's anything else you'd like to know or discuss, feel free to let me know!\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Jim! How can I assist you today?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///data/Northwind_small.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n"
     ]
    }
   ],
   "source": [
    "print(db.dialect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Category', 'Customer', 'CustomerCustomerDemo', 'CustomerDemographic', 'Employee', 'EmployeeTerritory', 'Order', 'OrderDetail', 'Product', 'Region', 'Shipper', 'Supplier', 'Territory']\n"
     ]
    }
   ],
   "source": [
    "print(db.get_usable_table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = create_sql_query_chain(llm, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(\"Id\") AS TotalCustomers\\nFROM Customer;'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = write_query.invoke({\"question\": \"How many customers are there\"}) \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(91,)]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/j1bs1q851k15cj5y777nxwph0000gn/T/ipykernel_3694/3528740886.py:1: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.\n",
      "  execute_query = QuerySQLDataBaseTool(db=db)\n"
     ]
    }
   ],
   "source": [
    "execute_query = QuerySQLDataBaseTool(db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_chain = write_query | execute_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(9,)]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_chain.invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a total of 9 employees.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many employees are there',\n",
       " 'query': 'SELECT COUNT(\"Id\") AS TotalEmployees\\nFROM Employee'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(query=write_query).invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many employees are there',\n",
       " 'query': 'SELECT COUNT(\"Id\") AS TotalEmployees FROM Employee;',\n",
       " 'result': 'SELECT COUNT(\"Id\") AS TotalEmployees FROM Employee;'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\")).invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Future Eclipses Get Ready for These Upcoming Eclipses! Solar Eclipses ... The date listed for each eclipse is the local date where the eclipse occurs. Lunar Eclipses ... Eclipse News More NASA News When's the next total solar eclipse? If seeing these amazing celestial spectacles makes you eager to see the next one, here is a short list of dates for the U.S. and Canada. For travelers, we've also highlighted three upcoming eclipses across the globe.. After the 2024 total solar eclipse, astronomy lovers are eager to know when the next extraterrestrial event will be visible in the U.S. Here is the schedule for the upcoming solar eclipses. The path of the next total solar eclipse to cross Alabama on August 12, 2045 From 1900 to 2100, the state of Alabama will have recorded a total of 87 solar eclipses, two of which are annular eclipses and four of which are total eclipses. The two annular solar eclipses occurred on April 7, 1940, and May 30, 1984, respectively. One total solar eclipse occurred on June 8, 1918, and three more ... The next partial solar eclipse in the U.S. will occur on Aug. 12, 2026, when northeastern states see a small partial solar eclipse at lunchtime — New York 10%, Boston 16% and Bar Harbor, Maine 24%.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "search.run(\"When will the next solar eclipse be?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
