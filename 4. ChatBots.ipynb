{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>LangChain for Generative AI</h1>\n",
    "<h1>ChatBot</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
    "\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import langchain_openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by print out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.3\n",
      "IPython version      : 9.2.0\n",
      "\n",
      "Compiler    : Clang 17.0.0 (clang-1700.0.13.3)\n",
      "OS          : Darwin\n",
      "Release     : 25.2.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 2563372cfdc6863bbd75970c20a0f7113c7e3c80\n",
      "\n",
      "langchain_core          : 0.3.62\n",
      "watermark               : 2.5.0\n",
      "langchain_text_splitters: 0.3.8\n",
      "numpy                   : 2.2.5\n",
      "langchain_openai        : 0.3.18\n",
      "matplotlib              : 3.10.3\n",
      "langchain_community     : 0.3.24\n",
      "pandas                  : 2.2.3\n",
      "langchain               : 0.3.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Romeo and Juliet This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Romeo and Juliet Author: William Shakespeare Release date: November .......'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = GutenbergLoader(\n",
    "    \"https://www.gutenberg.org/cache/epub/1513/pg1513.txt\"\n",
    ")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "extrait = ' '.join(document[0].page_content.split()[:100])\n",
    "display(extrait + \" .......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178720"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1024, # Each chunk is of size 1024\n",
    "    chunk_overlap=64 # Neigboring chunks overlap by 64 characters\n",
    ") \n",
    "\n",
    "texts = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I drew to part them, in the instant came\n",
      "\n",
      "\n",
      "The fiery Tybalt, with his sword prepar’d,\n",
      "\n",
      "\n",
      "Which, as he breath’d defiance to my ears,\n",
      "\n",
      "\n",
      "He swung about his head, and cut the winds,\n",
      "\n",
      "\n",
      "Who nothing hurt withal, hiss’d him in scorn.\n",
      "\n",
      "\n",
      "While we were interchanging thrusts and blows\n",
      "\n",
      "\n",
      "Came more and more, and fought on part and part,\n",
      "\n",
      "\n",
      "Till the Prince came, who parted either part.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY MONTAGUE.\n",
      "\n",
      "\n",
      "O where is Romeo, saw you him today?\n",
      "\n",
      "\n",
      "Right glad I am he was not at this fray.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "Madam, an hour before the worshipp’d sun\n",
      "\n",
      "\n",
      "Peer’d forth the golden window of the east,\n",
      "\n",
      "\n",
      "A troubled mind drave me to walk abroad,\n",
      "\n",
      "\n",
      "Where underneath the grove of sycamore\n",
      "\n",
      "\n",
      "That westward rooteth from this city side,\n",
      "\n",
      "\n",
      "So early walking did I see your son.\n",
      "\n",
      "\n",
      "Towards him I made, but he was ware of me,\n",
      "\n",
      "\n",
      "And stole into the covert of the wood.\n",
      "\n",
      "\n",
      "I, measuring his affections by my own,\n",
      "\n",
      "\n",
      "Which then most sought where most might not be found,\n",
      "\n",
      "\n",
      "Being one too many by my weary self,\n"
     ]
    }
   ],
   "source": [
    "print(texts[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being one too many by my weary self,\n",
      "\n",
      "\n",
      "Pursu’d my humour, not pursuing his,\n",
      "\n",
      "\n",
      "And gladly shunn’d who gladly fled from me.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MONTAGUE.\n",
      "\n",
      "\n",
      "Many a morning hath he there been seen,\n",
      "\n",
      "\n",
      "With tears augmenting the fresh morning’s dew,\n",
      "\n",
      "\n",
      "Adding to clouds more clouds with his deep sighs;\n",
      "\n",
      "\n",
      "But all so soon as the all-cheering sun\n",
      "\n",
      "\n",
      "Should in the farthest east begin to draw\n",
      "\n",
      "\n",
      "The shady curtains from Aurora’s bed,\n",
      "\n",
      "\n",
      "Away from light steals home my heavy son,\n",
      "\n",
      "\n",
      "And private in his chamber pens himself,\n",
      "\n",
      "\n",
      "Shuts up his windows, locks fair daylight out\n",
      "\n",
      "\n",
      "And makes himself an artificial night.\n",
      "\n",
      "\n",
      "Black and portentous must this humour prove,\n",
      "\n",
      "\n",
      "Unless good counsel may the cause remove.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "My noble uncle, do you know the cause?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MONTAGUE.\n",
      "\n",
      "\n",
      "I neither know it nor can learn of him.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "Have you importun’d him by any means?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MONTAGUE.\n",
      "\n",
      "\n",
      "Both by myself and many other friends;\n",
      "\n",
      "\n",
      "But he, his own affections’ counsellor,\n",
      "\n",
      "\n",
      "Is to himself—I will not say how true—\n"
     ]
    }
   ],
   "source": [
    "print(texts[11].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/j1bs1q851k15cj5y777nxwph0000gn/T/ipykernel_16930/3001161102.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    cache_folder=cache_dir\n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    texts, \n",
    "    embeddings, \n",
    "    persist_directory=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Romeo!\"\n",
    "\n",
    "docs = vectordb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the document\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo! My cousin Romeo! Romeo!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "He is wise,\n",
      "\n",
      "\n",
      "And on my life hath stol’n him home to bed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "He ran this way, and leap’d this orchard wall:\n",
      "\n",
      "\n",
      "Call, good Mercutio.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "Nay, I’ll conjure too.\n",
      "====================\n",
      "Romeo! My cousin Romeo! Romeo!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "He is wise,\n",
      "\n",
      "\n",
      "And on my life hath stol’n him home to bed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "He ran this way, and leap’d this orchard wall:\n",
      "\n",
      "\n",
      "Call, good Mercutio.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "Nay, I’ll conjure too.\n"
     ]
    }
   ],
   "source": [
    "# Check the content of the first document\n",
    "print(docs[0].page_content)\n",
    "print(\"=\"*20)\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a wrapper around the functionality of our vector database so we can search for similar documents in the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum. Keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Use the following pieces of context to answer the question at the end. \\n'\n",
      " \"If you don't know the answer, just say that you don't know, don't try to \"\n",
      " 'make up an answer. \\n'\n",
      " 'Use three sentences maximum. Keep the answer as concise as possible. \\n'\n",
      " 'Always say \"thanks for asking!\" at the end of the answer. \\n'\n",
      " '\\n'\n",
      " '{context}\\n'\n",
      " '\\n'\n",
      " 'Question: {question}\\n'\n",
      " '\\n'\n",
      " 'Helpful Answer:')\n"
     ]
    }
   ],
   "source": [
    "pprint(QA_CHAIN_PROMPT.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Juliet's family is the Capulets. Thanks for asking!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Juliets family?\"\n",
    "\n",
    "query_results_venice = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_venice['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Romeo is exiled for killing Tybalt, and Juliet is devastated by the news. Their separation and a series of tragic misunderstandings ultimately lead to both Romeo and Juliet taking their own lives. Thanks for asking!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What happens to Romeo and Juliet?\"\n",
    "query_results_romeo = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_romeo['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mercutio is a character in William Shakespeare\\'s play \"Romeo and Juliet.\" He is a close friend of Romeo and is known for his witty and playful nature. Thanks for asking!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Mercutio?\"\n",
    "query_results_romeo = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_romeo['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lr/j1bs1q851k15cj5y777nxwph0000gn/T/ipykernel_16930/948548302.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain_docs({\"query\": question})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It seems like you are quoting from a scene in \"Romeo and Juliet\" where Benvolio and Mercutio are looking for Romeo after he has leapt over the orchard wall. They believe he has gone home to bed, but he is actually hiding nearby. Thanks for asking!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does Romeo live?\"\n",
    "qa_chain_docs = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       \n",
    "                                       # Return source documents\n",
    "                                       return_source_documents=True,\n",
    "                                       \n",
    "                                            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain_docs({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo! My cousin Romeo! Romeo!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "He is wise,\n",
      "\n",
      "\n",
      "And on my life hath stol’n him home to bed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BENVOLIO.\n",
      "\n",
      "\n",
      "He ran this way, and leap’d this orchard wall:\n",
      "\n",
      "\n",
      "Call, good Mercutio.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MERCUTIO.\n",
      "\n",
      "\n",
      "Nay, I’ll conjure too.\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'][1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Does Romeo live in Romeo and Juliet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, Romeo does not live in \"Romeo and Juliet.\" In William Shakespeare\\'s tragic play, both Romeo and Juliet die. Romeo takes his own life after mistakenly believing that Juliet is dead, and Juliet subsequently takes her own life upon finding Romeo dead. Their deaths ultimately reconcile their feuding families, the Montagues and the Capulets.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
